""" from flask import Flask, request, jsonify, render_template
import ollama  # Utilisation d'un mod√®le IA local via Ollama

app = Flask(__name__)

# Charger le mod√®le une seule fois pour √©viter le rechargement √† chaque requ√™te
MODEL_NAME = "mistral"  # Change par "gemma" si besoin

def ask_ollama(prompt):
    response = ollama.chat(model=MODEL_NAME, messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

# Route pour afficher la page Web du chatbot
@app.route('/')
def home():
    return render_template('index.html')

# Route API pour g√©rer les requ√™tes du chatbot
@app.route('/chat', methods=['POST'])
def chat():
    user_input = request.json.get('message')
    if not user_input:
        return jsonify({"error": "Message vide"}), 400
    
    # Obtenir la r√©ponse de l'IA
    ai_response = ask_ollama(user_input)
    
    return jsonify({"response": ai_response})


if __name__ == '__main__':
    app.run(debug=True, threaded=True)  # Activation du mode multi-thread pour plus de rapidit√©
 """

""" from flask import Flask, request, jsonify
import ollama  # Mod√®le IA local
import telegram
from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, Dispatcher
import threading

TOKEN = "7776513976:AAGa30HezC-_dyXYc7ohljIM3ZyTGcX6dwk"  # Remplace par ton token
bot = telegram.Bot(token=TOKEN)

app = Flask(__name__)

# Initialiser le mod√®le IA
MODEL_NAME = "mistral"

def ask_ollama(prompt):
    response = ollama.chat(model=MODEL_NAME, messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

# G√©rer les messages Telegram
def start(update, context):
    update.message.reply_text("üëã Bonjour ! Je suis GDG-Yamoussoukro Bot ü§ñ. Pose-moi une question !")

def handle_message(update, context):
    user_text = update.message.text
    ai_response = ask_ollama(user_text)  # Appeler l'IA
    update.message.reply_text(ai_response)

# Configurer le webhook pour Telegram
@app.route(f"/{TOKEN}", methods=["POST"])
def webhook():
    update = telegram.Update.de_json(request.get_json(), bot)
    dispatcher.process_update(update)
    return "OK"

# Ajouter les handlers de commandes et messages
def setup_dispatcher(dp):
    dp.add_handler(CommandHandler("start", start))
    dp.add_handler(MessageHandler(Filters.text & ~Filters.command, handle_message))
    return dp

# Configurer le bot Telegram en arri√®re-plan
def run_telegram_bot():
    global dispatcher
    updater = Updater(TOKEN, use_context=True)
    dispatcher = setup_dispatcher(updater.dispatcher)
    updater.start_polling()
    updater.idle()

# Lancer le bot Telegram dans un thread s√©par√©
threading.Thread(target=run_telegram_bot).start()

if __name__ == "__main__":
    app.run(debug=True, threaded=True)
 """


""" from flask import Flask, request, jsonify, render_template
import ollama
import functools
import time
from flask_caching import Cache

app = Flask(__name__)
app.config["CACHE_TYPE"] = "simple"
cache = Cache(app)

# Charger le mod√®le une seule fois
MODEL_NAME = "mistral"  # Peut √™tre chang√© pour "gemma" si besoin
print(f"Chargement du mod√®le {MODEL_NAME}...")

def ask_ollama(prompt):
    Appelle Ollama avec une mise en cache pour √©viter les requ√™tes redondantes.
    @functools.lru_cache(maxsize=100)  # Cache m√©moire pour stocker les r√©ponses r√©centes
    def cached_request(p):
        response = ollama.chat(model=MODEL_NAME, messages=[{"role": "user", "content": p}])
        return response["message"]["content"]
    
    return cached_request(prompt)

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/chat', methods=['POST'])
def chat():
    user_input = request.json.get('message')
    if not user_input:
        return jsonify({"error": "Message vide"}), 400
    
    start_time = time.time()
    ai_response = ask_ollama(user_input)
    elapsed_time = time.time() - start_time
    print(f"R√©ponse g√©n√©r√©e en {elapsed_time:.2f}s")
    
    return jsonify({"response": ai_response})

if __name__ == '__main__':
    app.run(debug=True, threaded=True)
"""

""" import ollama
import time
from flask import Flask, request, jsonify, render_template
from flask_caching import Cache
from functools import lru_cache

app = Flask(__name__)
app.config["CACHE_TYPE"] = "simple"
cache = Cache(app)

MODEL_NAME = "gemma2:latest"
print(f"üîÑ Chargement du mod√®le {MODEL_NAME}...")

@lru_cache(maxsize=100)  # Cache les 100 derni√®res r√©ponses
def ask_ollama(prompt):
    
    Appelle Ollama avec mise en cache pour acc√©l√©rer les r√©ponses.
    
    response = ollama.chat(model=MODEL_NAME, messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/chat', methods=['POST'])
def chat():
    user_input = request.json.get('message')
    if not user_input:
        return jsonify({"error": "Message vide"}), 400
    
    start_time = time.time()
    ai_response = ask_ollama(user_input)
    elapsed_time = time.time() - start_time
    print(f"‚úÖ R√©ponse g√©n√©r√©e en {elapsed_time:.2f}s")
    
    return jsonify({"response": ai_response})

if __name__ == '__main__':
    app.run(debug=True, threaded=True)
"""

""" 
import ollama
import time
from flask import Flask, request, jsonify, render_template
from flask_caching import Cache
from functools import lru_cache

app = Flask(__name__)
app.config["CACHE_TYPE"] = "simple"
cache = Cache(app)

MODEL_NAME = "gemma2:latest"
print(f"üîÑ Chargement du mod√®le {MODEL_NAME}...")

@lru_cache(maxsize=100)  # Cache les 100 derni√®res r√©ponses
def ask_ollama(prompt):
    
    Appelle Ollama avec mise en cache pour acc√©l√©rer les r√©ponses.
    
    response = ollama.chat(model=MODEL_NAME, messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/chat', methods=['POST'])
def chat():
    user_input = request.json.get('message')
    if not user_input:
        return jsonify({"error": "Message vide"}), 400
    
    start_time = time.time()
    ai_response = ask_ollama(user_input)
    elapsed_time = time.time() - start_time
    print(f"‚úÖ R√©ponse g√©n√©r√©e en {elapsed_time:.2f}s")
    
    return jsonify({"response": ai_response})

if __name__ == '__main__':
    app.run(debug=True, threaded=True)
 """
""" 
import ollama
import time
from flask import Flask, request, jsonify, render_template
from flask_caching import Cache
from functools import lru_cache

app = Flask(__name__)
app.config["CACHE_TYPE"] = "simple"
cache = Cache(app)

MODEL_NAME = "gemma2:latest"
print(f"\U0001F504 Chargement du mod√®le {MODEL_NAME}...")

# Liste des sujets autoris√©s
SUJETS_AUTORISES = [
    "gyn√©cologie", "grossesse", "cycle menstruel", "contraception", "sant√© f√©minine",
    "pu√©riculture", "b√©b√©", "allaitement", "sommeil du b√©b√©", "d√©veloppement de l'enfant",
    "bonnes mani√®res", "√©tiquette", "savoir-vivre", "politesse"
]

def est_question_valide(question):
    
    V√©rifie si la question concerne les domaines de sp√©cialisation de Ze√Ønab.
    
    question = question.lower()
    return any(sujet in question for sujet in SUJETS_AUTORISES)

@lru_cache(maxsize=100)  # Cache les 100 derni√®res r√©ponses
def ask_ollama(prompt):
    
    Appelle Ollama avec mise en cache pour acc√©l√©rer les r√©ponses.
    
    response = ollama.chat(model=MODEL_NAME, messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/chat', methods=['POST'])
def chat():
    user_input = request.json.get('message')
    if not user_input:
        return jsonify({"error": "Message vide"}), 400
    
    if not est_question_valide(user_input):
        return jsonify({"response": "D√©sol√©, je suis sp√©cialis√©e en gyn√©cologie, pu√©riculture et bonnes mani√®res. Posez-moi une question dans ces domaines üòä."})
    
    start_time = time.time()
    ai_response = ask_ollama(user_input)
    elapsed_time = time.time() - start_time
    print(f"‚úÖ R√©ponse g√©n√©r√©e en {elapsed_time:.2f}s")
    
    return jsonify({"response": ai_response})

if __name__ == '__main__':
    app.run(debug=True, threaded=True)
"""


# -*- coding: utf-8 -*-
import time
import logging
from flask import Flask, request, jsonify, render_template
from flask_caching import Cache
from functools import lru_cache
import ollama

# Configuration du logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
app.config["CACHE_TYPE"] = "simple"
cache = Cache(app)

MODEL_NAME = "mistral"
logger.info(f"‚è≥ Chargement du mod√®le {MODEL_NAME}...")

# Liste des sujets autoris√©s
SUJETS_AUTORISES = [
    # Gyn√©cologie
    "gyn√©cologie", "grossesse", "cycle menstruel", "contraception", "sant√© f√©minine",
    "r√®gles", "menstruations", "douleurs menstruelles", "SPM", "syndrome pr√©menstruel", "fertilit√©", "ovulation", 
    "syndrome des ovaires polykystiques", "SOPK", "endom√©triose", "m√©nopause", "sympt√¥mes m√©nopause", 
    "traitements m√©nopause", "hormonoth√©rapie", "IST", "infections sexuellement transmissibles", "infection vaginale",
    "accouchement", "pr√©paration accouchement", "complications accouchement", "travail", "c√©sarienne", "post-partum", 
    "baby blues", "d√©pression post-partum", "p√©rin√©e", "r√©√©ducation p√©rin√©ale", "suivi gyn√©cologique", "frottis", 
    "mammographie", "hormones", "PMA", "procr√©ation m√©dicalement assist√©e", "FIV", "f√©condation in vitro", 
    "contr√¥le des naissances", "st√©rilet", "implant contraceptif", "pilule", "diaphragme", "am√©norrh√©e", "dysm√©norrh√©e", 
    "fibromes", "kystes ovariens", "vaginite", "mycose vaginale", "candidose", "infections urinaires", "vulvodynie",
    "douleurs pelviennes", "cancer du sein", "cancer de l'ut√©rus", "HPV", "papillomavirus",
    
    # Pu√©riculture
    "pu√©riculture", "b√©b√©", "allaitement", "biberon", "sevrage", "mont√©e de lait", "lait maternel", "lait infantile",
    "sommeil du b√©b√©", "cododo", "bercement", "troubles du sommeil b√©b√©", "d√©veloppement de l'enfant", 
    "r√©flexes du nouveau-n√©", "motricit√©", "d√©veloppement moteur", "marche", "premiers pas", "langage", "babillage", 
    "premiers mots", "alimentation du b√©b√©", "diversification alimentaire", "DME", "coliques", "reflux gastro-≈ìsophagien", 
    "allergies alimentaires", "ecz√©ma b√©b√©", "sant√© infantile", "fi√®vre", "vaccins", "calendrier vaccinal", 
    "maladies infantiles", "varicelle", "rougeole", "coqueluche", "bronchiolite", "otites", "pouss√©es dentaires",
    "√©ducation positive", "communication avec b√©b√©", "signes avec b√©b√©", "gestion des pleurs", "crises de col√®re",
    "terribles two", "autonomie", "apprentissage de la propret√©", "s√©curit√© b√©b√©", "chutes", "pr√©vention accidents domestiques",
    "p√©riode d'opposition", "attachement",
    
    # Bonnes mani√®res
    "bonnes mani√®res", "√©tiquette", "savoir-vivre", "politesse", "savoir-vivre en soci√©t√©", "respect des autres",
    "comment saluer", "pr√©sentation personnelle", "respect des a√Æn√©s", "courtoisie", "gestion des conflits", 
    "politesse avec les enfants", "communication non violente", "comment bien recevoir", "accueil des invit√©s", 
    "comportement en public", "comment parler en public", "au t√©l√©phone", "√©tiquette num√©rique", "codes sociaux",
    "remerciements", "excuses", "√©tiquette √† table", "comment tenir ses couverts", "conversation √† table", 
    "protocole", "bonnes mani√®res professionnelles", "savoir s'habiller", "√©tiquette dans les transports", 
    "respect de l'espace personnel"
]

def est_question_valide(question):
    """
    V√©rifie si la question traite d‚Äôun sujet autoris√©.
    """
    question = question.lower()
    mots_question = set(question.split())
    for sujet in SUJETS_AUTORISES:
        mots_sujet = set(sujet.split())
        if mots_sujet & mots_question:
            return True
    return False

@lru_cache(maxsize=100)
def ask_ollama(prompt):
    """
    Appelle Ollama avec un contexte sp√©cialis√© dans les domaines d√©finis.
    """
    contexte = (
        "Tu es une intelligence sp√©cialis√©e en gyn√©cologie, pu√©riculture et bonnes mani√®res. "
        "R√©ponds pr√©cis√©ment, avec un ton professionnel mais bienveillant. "
        "Donne des conseils pratiques et adapt√©s √† la question."
    )
    full_prompt = f"{contexte}\n\nQuestion: {prompt}"
    response = ollama.chat(model=MODEL_NAME, messages=[{"role": "user", "content": full_prompt}])
    return response["message"]["content"]

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/chat', methods=['POST'])
def chat():
    user_input = request.json.get('message')
    if not user_input:
        return jsonify({"error": "Message vide"}), 400

    if not est_question_valide(user_input):
        return jsonify({
            "response": "D√©sol√©, je suis sp√©cialis√©e en **gyn√©cologie, pu√©riculture et bonnes mani√®res**. "
                        "Posez-moi une question dans ces domaines üòä."
        })

    start_time = time.time()
    ai_response = ask_ollama(user_input)
    elapsed_time = time.time() - start_time
    logger.info(f"‚úÖ R√©ponse g√©n√©r√©e en {elapsed_time:.2f}s")

    return jsonify({"response": ai_response})

@app.route('/health')
def health():
    return jsonify({"status": "ok"})

if __name__ == '__main__':
    app.run(debug=True, threaded=True)
